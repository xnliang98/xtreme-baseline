data_dir: data/udpos/udpos_processed_maxlen128
model_type: xlmr
model_name_or_path: /cfs/cfs-pcgwsz/pcgwsz/geoffliang/ptms/xlm-roberta-large
output_dir: /cfs/cfs-pcgwsz/pcgwsz/geoffliang/udpos/outputs_3
labels: data/udpos/udpos_processed_maxlen128/labels.txt

config_name: ~
tokenizer_name: ~
cache_dir: ~
max_seq_length: 128
do_train: True
do_eval: True
do_predict: True
# do_predict_dev: True
init_checkpoint: ~
evaluate_during_training: False
do_lower_case: False
few_shot: -1

per_gpu_train_batch_size: 32
per_gpu_eval_batch_size: 32
gradient_accumulation_steps: 1
learning_rate: 2.0e-5
weight_decay: 0.0
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
num_train_epochs: 10
max_steps: -1
warmup_steps: -1

logging_steps: 500
save_steps: 500
save_only_best_checkpoint: False
eval_all_checkpoints: True
no_cuda: False
overwrite_output_dir: True
overwrite_cache: False
seed: 42

fp16: False
fp16_opt_level: O1
local_rank: -1
server_ip: ""
server_port: ""
predict_langs: 'af,ar,bg,de,el,en,es,et,eu,fa,fi,fr,he,hi,hu,id,it,ja,kk,ko,mr,nl,pt,ru,ta,te,th,tl,tr,ur,vi,yo,zh,lt,pl,uk,ro'
# predict_langs: 'vi,th'
# train_langs: en
train_langs: 'af,ar,bg,de,el,en,es,et,eu,fa,fi,fr,he,hi,hu,id,it,ja,kk,ko,mr,nl,pt,ru,ta,te,th,tl,tr,ur,vi,yo,zh,lt,pl,uk,ro'
log_file: training.log
eval_patience: -1

